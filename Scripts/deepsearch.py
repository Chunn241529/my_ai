
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup

import requests
import json
from rich.markdown import Markdown


# import scripts
from commands import *
from file import *
from image import *

# Kh·ªüi t·∫°o console t·ª´ Rich
console = Console()


# URL c·ªßa Ollama API
OLLAMA_API_URL = "http://localhost:11434/api/generate"


model_gemma = "gemma3:latest"
model_qwen = "qwen2.5-coder:latest"

# C√°c h√†m t·ª´ search.py
def search_web(query, max_results=5):
    results = []
    with DDGS() as ddgs:
        for r in ddgs.text(query, max_results=max_results):
            results.append({
                'title': r['title'],
                'url': r['href'],
                'snippet': r['body']
            })
    return results

def extract_content(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        paragraphs = soup.find_all('p')
        content = " ".join([p.get_text() for p in paragraphs])
        return content[:12000]
    except Exception as e:
        return f"Error fetching {url}: {str(e)}"




def reason_with_ollama(query, context):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nTh√¥ng tin: {context}\nH√£y suy lu·∫≠n v√† tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi ch√≠nh {query}. T·∫≠p trung ho√†n to√†n v√†o tr·ªçng t√¢m c√¢u h·ªèi, b·ªè qua th√¥ng tin kh√¥ng li√™n quan."
    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def evaluate_answer(query, answer):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    eval_prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nC√¢u tr·∫£ l·ªùi: {answer}\nC√¢u tr·∫£ l·ªùi n√†y ƒë√£ ƒë·ªß ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi cho C√¢u h·ªèi ch√≠nh: {query} ch∆∞a? ƒê·∫ßu ti√™n, tr·∫£ l·ªùi 'ƒê√£ ƒë·ªß' n·∫øu c√¢u tr·∫£ l·ªùi cung c·∫•p ƒë·∫ßy ƒë·ªß c√¢u tr·∫£ l·ªùi cho C√¢u h·ªèi ch√≠nh: {query}, ho·∫∑c 'Ch∆∞a ƒë·ªß' n·∫øu thi·∫øu th√¥ng tin c·∫ßn thi·∫øt. N·∫øu 'ƒê√£ ƒë·ªß', kh√¥ng ƒë·ªÅ xu·∫•t g√¨ th√™m. N·∫øu 'Ch∆∞a ƒë·ªß', ƒë·ªÅ xu·∫•t CH·ªà 1 truy v·∫•n c·ª• th·ªÉ, li√™n quan tr·ª±c ti·∫øp ƒë·∫øn C√¢u h·ªèi ch√≠nh: {query}, trong ph·∫ßn 'ƒê·ªÅ xu·∫•t truy v·∫•n:' v·ªõi ƒë·ªãnh d·∫°ng:\nƒê·ªÅ xu·∫•t truy v·∫•n:\n* \"truy v·∫•n\"\nV√≠ d·ª•:\nCh∆∞a ƒë·ªß\nƒê·ªÅ xu·∫•t truy v·∫•n:\n* \"{query}\"\nƒê·∫£m b·∫£o lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng 'ƒê√£ ƒë·ªß' ho·∫∑c 'Ch∆∞a ƒë·ªß'."
    payload = {
        "model": model_gemma,
        "prompt": eval_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def summarize_answers(query, all_answers):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    summary_prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nD∆∞·ªõi ƒë√¢y l√† c√°c th√¥ng tin ƒë√£ thu th·∫≠p:\n" + "\n".join([f"{q}: {a}" for q, a in all_answers.items()]) + f"\nT·ªïng h·ª£p th√†nh m·ªôt c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, logic v√† ƒë·∫ßy ƒë·ªß nh·∫•t cho C√¢u h·ªèi ch√≠nh: {query}, t·∫≠p trung v√†o tr·ªçng t√¢m."
    payload = {
        "model": model_gemma,
        "prompt": summary_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def extract_queries(text):
    queries = set()
    lines = text.split('\n')
    in_query_section = False
    
    for i, line in enumerate(lines):
        line_clean = line.strip().lower()
        if line_clean.startswith('ƒë·ªÅ xu·∫•t truy v·∫•n:') or line_clean.startswith('**ƒë·ªÅ xu·∫•t truy v·∫•n:**'):
            in_query_section = True
        elif in_query_section and (not line.strip() or not line.strip().startswith('*')):
            in_query_section = False
        
        if in_query_section:
            if i + 1 < len(lines) and not lines[i].strip().startswith('*') and lines[i].strip().startswith('ƒê·ªÅ xu·∫•t truy v·∫•n:'):
                next_line = lines[i + 1].strip()
                if next_line and not next_line.startswith('Truy v·∫•n t·ª´') and not next_line.startswith('ƒê√°nh gi√°:'):
                    clean_query = next_line.strip('"').strip('*').strip()
                    if clean_query:
                        analys_prompt_stream = analys_prompt(clean_query)

                        full_query=""
                        for part in analys_prompt_stream:
                            if part is not None:
                                full_query += part


                        queries.add(full_query)
            elif line.strip().startswith('*'):
                clean_query = line.strip()[1:].strip().strip('"').strip()
                if clean_query:
                    queries.add(clean_query)
    
    return list(queries)[:1]


def analys_prompt(query):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"From the given query, translate it to English if necessary, then provide exactly one concise English search query (no explanations, no extra options) that a user would use to find relevant information on the web. Query: {query}"
    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def deepsearch(initial_query, max_iterations=3):
    current_queries = []
    accumulated_context = ""
    iteration = 0
    all_answers = {}

    analys_prompt_stream = analys_prompt(initial_query)

    full_query=""
    for part in analys_prompt_stream:
        if part is not None:
            full_query += part

    current_queries.append(full_query)

    while iteration < max_iterations and current_queries:
        current_query = current_queries.pop(0)
        
        console.print("\n")
        console.print(f"[cyan]T√¨m ki·∫øm: {current_query} üîç[/cyan]")
        console.print("\n")

        search_results = search_web(current_query)
        if not search_results:
            all_answers[current_query] = "Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan."
            console.print(f"  [red]K·∫øt qu·∫£: Kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan. üòï[/red]")
            continue

        for result in search_results:
            content = extract_content(result['url'])
            accumulated_context += f"\nNgu·ªìn: {result['url']}\n{content}\n"

        # Thu th·∫≠p to√†n b·ªô ph·∫£n h·ªìi t·ª´ reason_with_ollama
        answer_stream = reason_with_ollama(initial_query, accumulated_context)
        full_answer = ""
        for part in answer_stream:
            if part is not None:
                full_answer += part
        all_answers[current_query] = full_answer
        console.print(Markdown(full_answer), soft_wrap=True, end="")

        # Tr√≠ch xu·∫•t truy v·∫•n t·ª´ suy lu·∫≠n nh∆∞ng kh√¥ng hi·ªÉn th·ªã
        new_queries_from_reasoning = extract_queries(full_answer)
        # console.print(f"  [blue]Truy v·∫•n t·ª´ suy lu·∫≠n: {new_queries_from_reasoning} ü§ì[/blue]")  # ·∫®n d√≤ng n√†y

        # Thu th·∫≠p to√†n b·ªô ph·∫£n h·ªìi t·ª´ evaluate_answer
        evaluation_stream = evaluate_answer(initial_query, full_answer)
        full_evaluation = ""
        for part in evaluation_stream:
            if part is not None:
                full_evaluation += part
        console.print(f"[magenta]ƒê√°nh gi√°: {full_evaluation}[/magenta]")
        
        if "ƒë√£ ƒë·ªß" in full_evaluation.lower():
            console.print("[bold green]Th√¥ng tin ƒë√£ ƒë·ªß, kh√¥ng c·∫ßn t√¨m th√™m! üéâ[/bold green]")
            # if new_queries_from_reasoning:
            #     console.print(f"  [yellow]L∆∞u √Ω: C√≥ truy v·∫•n t·ª´ suy lu·∫≠n ({new_queries_from_reasoning}) nh∆∞ng b·ªã b·ªè qua v√¨ ƒë√°nh gi√° 'ƒê√£ ƒë·ªß'.[/yellow]")  # ·∫®n d√≤ng n√†y
            break
        elif "ch∆∞a ƒë·ªß" in full_evaluation.lower():
            new_queries_from_evaluation = extract_queries(full_evaluation)
            # console.print(f"  [blue]Truy v·∫•n t·ª´ ƒë√°nh gi√°: {new_queries_from_evaluation} üîÑ[/blue]")  # ·∫®n d√≤ng n√†y
            relevant_query = new_queries_from_evaluation[0] if new_queries_from_evaluation else (new_queries_from_reasoning[0] if new_queries_from_reasoning else None)
            if relevant_query and relevant_query not in current_queries and relevant_query not in all_answers:
                current_queries.append(relevant_query)
                # console.print(f"  [cyan]Truy v·∫•n ƒë∆∞·ª£c th√™m: [{relevant_query}] üöÄ[/cyan]")  # ·∫®n d√≤ng n√†y
            else:
                console.print("[yellow]Kh√¥ng c√≥ truy v·∫•n m·ªõi ph√π h·ª£p ƒë·ªÉ ti·∫øp t·ª•c. ü§î[/yellow]")
            iteration += 1
        else:
            console.print(f"[red]ƒê√°nh gi√° kh√¥ng r√µ r√†ng: {full_evaluation} ‚ùì[/red]")
            new_queries_from_evaluation = extract_queries(full_evaluation)
            relevant_query = new_queries_from_evaluation[0] if new_queries_from_evaluation else (new_queries_from_reasoning[0] if new_queries_from_reasoning else None)
            if relevant_query and relevant_query not in current_queries and relevant_query not in all_answers:
                current_queries.append(relevant_query)
                # console.print(f"  [cyan]Truy v·∫•n t·ª´ ƒë√°nh gi√° kh√¥ng r√µ r√†ng: [{relevant_query}] üîÑ[/cyan]")  # ·∫®n d√≤ng n√†y
            else:
                current_queries.append(current_query)
            iteration += 1

    if iteration >= max_iterations:
        console.print(f"\n[bold red]ƒê√£ ƒë·∫°t gi·ªõi h·∫°n {max_iterations} l·∫ßn t√¨m ki·∫øm. ‚è≥[/bold red]")
    else:
        console.print("\n[bold green]ƒê√£ ho√†n th√†nh t√¨m ki·∫øm! üåü[/bold green]")
    
    # Thu th·∫≠p to√†n b·ªô ph·∫£n h·ªìi t·ª´ summarize_answers
    summary_stream = summarize_answers(initial_query, all_answers)
    final_answer = ""
    for part in summary_stream:
        if part is not None:
            final_answer += part
    return f"\n{final_answer}"

#H√†m test 

# query = "So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh AI l·ªõn hi·ªán t·∫°i"
# console.print(deepsearch(query))