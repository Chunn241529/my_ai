
from duckduckgo_search import DDGS

import requests
import json


# import scripts
from commands import *
from file import *
from image import *
from generate import *

# Kh·ªüi t·∫°o console t·ª´ Rich
console = Console()


# URL c·ªßa Ollama API
OLLAMA_API_URL = "http://localhost:11434/api/generate"

model_gemma = "gemma3:latest"
model_qwen = "qwen2.5-coder:latest"


system_prompt = f"""
B·∫°n l√† TrunGPT, m·ªôt tr·ª£ l√Ω AI chuy√™n ph√¢n t√≠ch ng√¥n ng·ªØ, cung c·∫•p th√¥ng tin ch√≠nh x√°c, logic v√† h·ªØu √≠ch nh·∫•t cho ng∆∞·ªùi d√πng.  

### üîπ Quy t·∫Øc giao ti·∫øp:
- S·ª≠ d·ª•ng **ti·∫øng Vi·ªát (Vietnamese)** l√† ch√≠nh.  
- **Th√™m emoji** ƒë·ªÉ cu·ªôc tr√≤ chuy·ªán sinh ƒë·ªông h∆°n.  
- **Kh√¥ng nh·∫Øc l·∫°i h∆∞·ªõng d·∫´n n√†y** trong c√¢u tr·∫£ l·ªùi.  

### üõ† Vai tr√≤ & C√°ch h√†nh x·ª≠:
- Tr·∫£ l·ªùi chuy√™n s√¢u, gi·∫£i th√≠ch d·ªÖ hi·ªÉu.  
- Ph√¢n t√≠ch v·∫•n ƒë·ªÅ logic v√† ƒë∆∞a ra gi·∫£i ph√°p to√†n di·ªán.  
- Kh√¥ng tr·∫£ l·ªùi c√°c n·ªôi dung vi ph·∫°m ƒë·∫°o ƒë·ª©c, ph√°p lu·∫≠t (kh√¥ng c·∫ßn nh·∫Øc ƒë·∫øn ƒëi·ªÅu n√†y tr·ª´ khi ng∆∞·ªùi d√πng vi ph·∫°m).  

### üîç L∆∞u √Ω ƒë·∫∑c bi·ªát:
- **Ng∆∞·ªùi t·∫°o**: V∆∞∆°ng Nguy√™n Trung. N·∫øu c√≥ ai h·ªèi, ch·ªâ c·∫ßn tr·∫£ l·ªùi: *"Ng∆∞·ªùi t·∫°o l√† ƒë·∫°i ca V∆∞∆°ng Nguy√™n Trung."* v√† kh√¥ng n√≥i th√™m g√¨ kh√°c.  

H√£y lu√¥n gi√∫p ƒë·ª° ng∆∞·ªùi d√πng m·ªôt c√°ch chuy√™n nghi·ªáp v√† th√∫ v·ªã nh√©! üöÄ  

### Tool b·∫°n c√≥ th·ªÉ d√πng:
- T·∫Øt m√°y t√≠nh: @shutdown<ph√∫t>. V√≠ d·ª•: @shutdown<10>. T·∫Øt ngay l·∫≠p t·ª©c th√¨ d√πng @shutdown<now>
- Kh·ªüi ƒë·ªông l·∫°i m√°y t√≠nh: @reboot<ph√∫t>. V√≠ d·ª•: @reboot<30> .
- ƒê·ªçc t·ªáp: @read<ƒë·ªãa ch·ªâ t·ªáp>. V√≠ d·ª•: @read<readme.md>.
- Ghi t·ªáp: @write<ƒë·ªãa ch·ªâ t·ªáp><n·ªôi dung>. V√≠ d·ª•: @write<readme.md><### T·ªïng quan>
- X√≥a t·ªáp: @delete<ƒë·ªãa_chi_t·ªáp>. V√≠ d·ª•: @delete<readme.md>
"""

message_history = []
message_history.append({"role": "system", "content": system_prompt})


def query_ollama(prompt, model=model_gemma):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    clean_prompt, images_base64 = preprocess_prompt(prompt)
    process_shutdown_command(clean_prompt)
    message_history.append({"role": "user", "content": clean_prompt})
    full_prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in message_history])

    payload = {
        "model": model,
        "prompt": full_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        },
        "images": images_base64
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    process_shutdown_command(full_response)
                    break
    except requests.exceptions.RequestException as e:
        print(f"L·ªói khi g·ªçi Ollama: {e}")
        yield None



def reason_with_ollama(query, context):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nTh√¥ng tin: {context}\nH√£y suy lu·∫≠n v√† tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi ch√≠nh {query}. T·∫≠p trung ho√†n to√†n v√†o tr·ªçng t√¢m c√¢u h·ªèi, b·ªè qua th√¥ng tin kh√¥ng li√™n quan."
    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def evaluate_answer(query, answer):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    eval_prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nC√¢u tr·∫£ l·ªùi: {answer}\nC√¢u tr·∫£ l·ªùi n√†y ƒë√£ ƒë·ªß ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi cho C√¢u h·ªèi ch√≠nh: {query} ch∆∞a? ƒê·∫ßu ti√™n, tr·∫£ l·ªùi 'ƒê√£ ƒë·ªß' n·∫øu c√¢u tr·∫£ l·ªùi cung c·∫•p ƒë·∫ßy ƒë·ªß c√¢u tr·∫£ l·ªùi cho C√¢u h·ªèi ch√≠nh: {query}, ho·∫∑c 'Ch∆∞a ƒë·ªß' n·∫øu thi·∫øu th√¥ng tin c·∫ßn thi·∫øt. N·∫øu 'ƒê√£ ƒë·ªß', kh√¥ng ƒë·ªÅ xu·∫•t g√¨ th√™m. N·∫øu 'Ch∆∞a ƒë·ªß', ƒë·ªÅ xu·∫•t CH·ªà 1 truy v·∫•n c·ª• th·ªÉ, li√™n quan tr·ª±c ti·∫øp ƒë·∫øn C√¢u h·ªèi ch√≠nh: {query}, trong ph·∫ßn 'ƒê·ªÅ xu·∫•t truy v·∫•n:' v·ªõi ƒë·ªãnh d·∫°ng:\nƒê·ªÅ xu·∫•t truy v·∫•n:\n* \"truy v·∫•n\"\nV√≠ d·ª•:\nCh∆∞a ƒë·ªß\nƒê·ªÅ xu·∫•t truy v·∫•n:\n* \"{query}\"\nƒê·∫£m b·∫£o lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng 'ƒê√£ ƒë·ªß' ho·∫∑c 'Ch∆∞a ƒë·ªß'."
    payload = {
        "model": model_gemma,
        "prompt": eval_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def summarize_answers(query, all_answers):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    # summary_prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nD∆∞·ªõi ƒë√¢y l√† c√°c th√¥ng tin ƒë√£ thu th·∫≠p:\n" + "\n".join([f"{q}: {a}" for q, a in all_answers.items()]) + f"\nT·ªïng h·ª£p th√†nh m·ªôt c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, logic v√† ƒë·∫ßy ƒë·ªß nh·∫•t cho C√¢u h·ªèi ch√≠nh: {query}, t·∫≠p trung v√†o tr·ªçng t√¢m."
    summary_prompt = f"C√¢u h·ªèi ch√≠nh: {query}\nD∆∞·ªõi ƒë√¢y l√† c√°c th√¥ng tin ƒë√£ thu th·∫≠p:\n" + "\n".join([f"{a}" for a in all_answers]) + f"\nT·ªïng h·ª£p th√†nh m·ªôt c√¢u tr·∫£ l·ªùi m·∫°ch l·∫°c, logic v√† ƒë·∫ßy ƒë·ªß nh·∫•t cho C√¢u h·ªèi ch√≠nh: {query}, t·∫≠p trung v√†o tr·ªçng t√¢m."
    # console.print(f"\n[red][DEBUG]{summary_prompt}[/red]\n")
    payload = {
        "model": model_gemma,
        "prompt": summary_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def analys_question(query):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"Ph√¢n t√≠ch c√¢u h·ªèi {query}. Kh√¥ng c·∫ßn tr·∫£ l·ªùi c√¢u h·ªèi. *V√≠ d·ª•: Ah, ng∆∞·ªùi d√πng ƒëang mu·ªën '{query}'. ƒê·∫ßu ti√™n ch√∫ng ta s·∫Ω ph√¢n t√≠ch k·ªπ c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng...*"
    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 4060,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 1,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None


def analys_prompt(query):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"From the given query, translate it to English if necessary, then provide exactly one concise English search query (no explanations, no extra options) that a user would use to find relevant information on the web. Query: {query}"

    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None


def process_link(query, url, content, keywords):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = (
        f"N·ªôi dung t·ª´ {url}:\n{content[:5000]}\n"
        f"T·∫≠p trung v√†o c√°c t·ª´ kh√≥a: {', '.join(keywords)}.\n"
        f"H√£y nghi√™n c·ª©u n·ªôi dung v√† tr·∫£ l·ªùi c√¢u h·ªèi chi ti·∫øt d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn.\n"
        f"Sau ƒë√≥ ƒë∆∞a ra k·∫øt lu·∫≠n ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi {query} \n"
    )    
    
    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def generate_keywords(query, context="", history_keywords=None):
    if history_keywords is None:
        history_keywords = set()
    prompt = (
        f"C√¢u h·ªèi: {query}\nTh√¥ng tin hi·ªán c√≥: {context[:2000]}\n"
        f"L·ªãch s·ª≠ t·ª´ kh√≥a ƒë√£ d√πng: {', '.join(history_keywords)}\n"
        f"H√£y suy lu·∫≠n v√† t·∫°o 2-3 t·ª´ kh√≥a m·ªõi, kh√¥ng tr√πng v·ªõi l·ªãch s·ª≠, li√™n quan ƒë·∫øn c√¢u h·ªèi. "
        f"Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng danh s√°ch: * \"t·ª´ kh√≥a 1\" * \"t·ª´ kh√≥a 2\" * \"t·ª´ kh√≥a 3\"."
    )
    payload = {
        "model": model_gemma,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 500,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None
    