
from duckduckgo_search import DDGS

import requests
import json


# import scripts
from commands import *
from file import *
from image import *
from generate import *

# Khá»Ÿi táº¡o console tá»« Rich
console = Console()


# URL cá»§a Ollama API
OLLAMA_API_URL = "http://localhost:11434/api/generate"

model_gemma = "gemma3:latest"
model_qwen = "qwen2.5-coder:latest"

model_curent = model_gemma

system_prompt = f"""
Báº¡n lÃ  TrunGPT, má»™t trá»£ lÃ½ AI chuyÃªn phÃ¢n tÃ­ch ngÃ´n ngá»¯, cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c, logic vÃ  há»¯u Ã­ch nháº¥t cho ngÆ°á»i dÃ¹ng.  

### ğŸ”¹ Quy táº¯c giao tiáº¿p:
- Sá»­ dá»¥ng **tiáº¿ng Viá»‡t (Vietnamese)** lÃ  chÃ­nh.  
- **ThÃªm emoji** Ä‘á»ƒ cuá»™c trÃ² chuyá»‡n sinh Ä‘á»™ng hÆ¡n.  
- **KhÃ´ng nháº¯c láº¡i hÆ°á»›ng dáº«n nÃ y** trong cÃ¢u tráº£ lá»i.  

### ğŸ›  Vai trÃ² & CÃ¡ch hÃ nh xá»­:
- Tráº£ lá»i chuyÃªn sÃ¢u, giáº£i thÃ­ch dá»… hiá»ƒu.  
- PhÃ¢n tÃ­ch váº¥n Ä‘á» logic vÃ  Ä‘Æ°a ra giáº£i phÃ¡p toÃ n diá»‡n.  
- KhÃ´ng tráº£ lá»i cÃ¡c ná»™i dung vi pháº¡m Ä‘áº¡o Ä‘á»©c, phÃ¡p luáº­t (khÃ´ng cáº§n nháº¯c Ä‘áº¿n Ä‘iá»u nÃ y trá»« khi ngÆ°á»i dÃ¹ng vi pháº¡m).  

### ğŸ” LÆ°u Ã½ Ä‘áº·c biá»‡t:
- **NgÆ°á»i táº¡o**: VÆ°Æ¡ng NguyÃªn Trung. Náº¿u cÃ³ ai há»i, chá»‰ cáº§n tráº£ lá»i: *"NgÆ°á»i táº¡o lÃ  Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung."* vÃ  khÃ´ng nÃ³i thÃªm gÃ¬ khÃ¡c.  

HÃ£y luÃ´n giÃºp Ä‘á»¡ ngÆ°á»i dÃ¹ng má»™t cÃ¡ch chuyÃªn nghiá»‡p vÃ  thÃº vá»‹ nhÃ©! ğŸš€  

### Tool báº¡n cÃ³ thá»ƒ dÃ¹ng:
- Táº¯t mÃ¡y tÃ­nh: @shutdown<phÃºt>. VÃ­ dá»¥: @shutdown<10>. Táº¯t ngay láº­p tá»©c thÃ¬ dÃ¹ng @shutdown<now>
- Khá»Ÿi Ä‘á»™ng láº¡i mÃ¡y tÃ­nh: @reboot<phÃºt>. VÃ­ dá»¥: @reboot<30> .
- Äá»c tá»‡p: @read<Ä‘á»‹a chá»‰ tá»‡p>. VÃ­ dá»¥: @read<readme.md>.
- Ghi tá»‡p: @write<Ä‘á»‹a chá»‰ tá»‡p><ná»™i dung>. VÃ­ dá»¥: @write<readme.md><### Tá»•ng quan>
- XÃ³a tá»‡p: @delete<Ä‘á»‹a_chi_tá»‡p>. VÃ­ dá»¥: @delete<readme.md>
"""

message_history = []
message_history.append({"role": "system", "content": system_prompt})


def query_ollama(prompt, model=model_curent):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    clean_prompt, images_base64 = preprocess_prompt(prompt)
    process_shutdown_command(clean_prompt)
    message_history.append({"role": "user", "content": clean_prompt})
    full_prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in message_history])

    payload = {
        "model": model,
        "prompt": full_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        },
        "images": images_base64
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    process_shutdown_command(full_response)
                    break
    except requests.exceptions.RequestException as e:
        print(f"Lá»—i khi gá»i Ollama: {e}")
        yield None


def analys_question(query):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = f"""
    PhÃ¢n tÃ­ch cÃ¢u há»i: '{query}'.  
    - KhÃ´ng tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i.  
    - Chá»‰ táº­p trung vÃ o:  
      * Ã Ä‘á»‹nh cá»§a ngÆ°á»i dÃ¹ng (há» muá»‘n gÃ¬?).  
      * CÃ¡c cÃ¡ch hiá»ƒu khÃ¡c nhau cá»§a cÃ¢u há»i.  
    - Äi tháº³ng vÃ o phÃ¢n tÃ­ch, khÃ´ng thÃªm lá»i nháº­n xÃ©t hoáº·c má»Ÿ Ä‘áº§u thá»«a thÃ£i.  
    - Giá»¯ giá»ng Ä‘iá»‡u tá»± nhiÃªn, ngáº¯n gá»n. VÃ­ dá»¥: 'CÃ¢u "{query}" cho tháº¥y ngÆ°á»i dÃ¹ng muá»‘n [Ã½ Ä‘á»‹nh]. NÃ³ cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c hiá»ƒu lÃ  [cÃ¡ch hiá»ƒu khÃ¡c].'\n
    - Náº¿u cÃ¢u há»i KhÃ´ng kháº£ thi, hÃ£y tráº£ lá»i "KhÃ³ nha bro..."
    """
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 1000,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.7,  # Giáº£m Ä‘á»ƒ giáº£m ngáº«u nhiÃªn
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None


def reason_with_ollama(query, context):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = (
        f"CÃ¢u há»i chÃ­nh: {query}\n"
        f"ThÃ´ng tin: {context}\n"
        f"HÃ£y reasoning vÃ  tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i chÃ­nh '{query}' dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p. Thá»±c hiá»‡n theo cÃ¡c bÆ°á»›c sau:\n"
        f"1. PhÃ¢n tÃ­ch thÃ´ng tin: XÃ¡c Ä‘á»‹nh cÃ¡c dá»¯ kiá»‡n quan trá»ng liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u há»i.\n"
        f"2. Suy luáº­n logic: Dá»±a trÃªn cÃ¡c dá»¯ kiá»‡n Ä‘Ã£ phÃ¢n tÃ­ch, Ä‘Æ°a ra láº­p luáº­n há»£p lÃ½ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i.\n"
    )
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.7,  # Giáº£m temperature Ä‘á»ƒ tÄƒng tÃ­nh logic vÃ  chÃ­nh xÃ¡c
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def evaluate_answer(query, answer):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    eval_prompt = (
        f"CÃ¢u tráº£ lá»i: {answer}\n"
        f"CÃ¢u ban Ä‘áº§u: {query}\n"
        f"CÃ¢u tráº£ lá»i nÃ y Ä‘Ã£ Ä‘á»§ Ä‘á»ƒ tráº£ lá»i Ä‘áº§y Ä‘á»§ CÃ¢u ban Ä‘áº§u chÆ°a? \n"
        f"Tráº£ lá»i báº¯t Ä‘áº§u báº±ng 'ÄÃ£ Ä‘á»§' náº¿u thÃ´ng tin Ä‘áº§y Ä‘á»§, hoáº·c 'ChÆ°a Ä‘á»§' náº¿u thiáº¿u thÃ´ng tin cáº§n thiáº¿t. \n"
        f"Náº¿u 'ÄÃ£ Ä‘á»§', khÃ´ng thÃªm gÃ¬ ná»¯a. \n"
        f"Náº¿u 'ChÆ°a Ä‘á»§', thÃªm pháº§n 'Äá» xuáº¥t truy váº¥n:' vá»›i CHá»ˆ 1 truy váº¥n cá»¥ thá»ƒ báº±ng tiáº¿ng Anh, ngáº¯n gá»n, dáº¡ng cÃ¢u lá»‡nh tÃ¬m kiáº¿m (khÃ´ng pháº£i cÃ¢u há»i), liÃªn quan trá»±c tiáº¿p Ä‘áº¿n CÃ¢u ban Ä‘áº§u, theo Ä‘á»‹nh dáº¡ng:\n"
        f"Äá» xuáº¥t truy váº¥n:\n* \"tá»« khÃ³a hoáº·c cá»¥m tá»« tÃ¬m kiáº¿m cá»¥ thá»ƒ\"\n"
        f"VÃ­ dá»¥:\nChÆ°a Ä‘á»§\nÄá» xuáº¥t truy váº¥n:\n* \"details about {query}\"\n"
        f"Äáº£m báº£o luÃ´n báº¯t Ä‘áº§u báº±ng 'ÄÃ£ Ä‘á»§' hoáº·c 'ChÆ°a Ä‘á»§', vÃ  truy váº¥n pháº£i lÃ  cá»¥m tá»« tÃ¬m kiáº¿m, khÃ´ng pháº£i cÃ¢u há»i."
    )

    payload = {
        "model": model_curent,
        "prompt": eval_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def summarize_answers(query, all_answers):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    summary_prompt = f"""
    CÃ¢u há»i chÃ­nh: '{query}'  
    CÃ¡c thÃ´ng tin Ä‘Ã£ thu tháº­p: {'\n'.join([f'- {a}' for a in all_answers])}  
    Tá»•ng há»£p táº¥t cáº£ thÃ´ng tin trÃªn thÃ nh má»™t cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§ nháº¥t cho cÃ¢u há»i '{query}'.  
    - Táº­p trung vÃ o trá»ng tÃ¢m cá»§a cÃ¢u há»i.   
    - Sá»­ dá»¥ng giá»ng Ä‘iá»‡u tá»± nhiÃªn, dá»… hiá»ƒu.  
    VÃ­ dá»¥: Náº¿u cÃ¢u há»i lÃ  'LÃ m sao Ä‘á»ƒ há»c tiáº¿ng Anh nhanh?' vÃ  thÃ´ng tin gá»“m: 'Há»c tá»« vá»±ng má»—i ngÃ y', 'Xem phim tiáº¿ng Anh', thÃ¬ tá»•ng há»£p thÃ nh: 'Äá»ƒ há»c tiáº¿ng Anh nhanh, báº¡n nÃªn há»c tá»« vá»±ng má»—i ngÃ y vÃ  xem phim tiáº¿ng Anh thÆ°á»ng xuyÃªn.'  
    """
    payload = {
        "model": model_curent,
        "prompt": summary_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,  # Giá»›i háº¡n Ä‘á»™ dÃ i Ä‘á»ƒ ngáº¯n gá»n
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.5,  # Giáº£m Ä‘á»ƒ tÄƒng tÃ­nh logic
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None


def analys_prompt(query):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = f"From the given query, translate it to English if necessary, then provide exactly one concise English search query (no explanations, no extra options) that a user would use to find relevant information on the web. Query: {query}"

    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None


def process_link(query, url, content, keywords):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = (
        f"Ná»™i dung tá»« {url}:\n{content[:12000]}\n"
        f"Táº­p trung vÃ o cÃ¡c tá»« khÃ³a: {', '.join(keywords)}.\n"
        f"HÃ£y suy luáº­n vÃ  tráº£ lá»i cÃ¢u há»i '{query}' dá»±a trÃªn ná»™i dung Ä‘Æ°á»£c cung cáº¥p, thá»±c hiá»‡n theo cÃ¡c bÆ°á»›c sau:\n"
        f"1. PhÃ¢n tÃ­ch ná»™i dung: XÃ¡c Ä‘á»‹nh cÃ¡c thÃ´ng tin quan trá»ng liÃªn quan Ä‘áº¿n tá»« khÃ³a vÃ  cÃ¢u há»i. LÆ°u Ã½ cÃ¡c dá»¯ kiá»‡n cá»¥ thá»ƒ, sá»‘ liá»‡u, hoáº·c Ã½ chÃ­nh.\n"
        f"2. LiÃªn káº¿t vá»›i tá»« khÃ³a: ÄÃ¡nh giÃ¡ cÃ¡ch cÃ¡c tá»« khÃ³a xuáº¥t hiá»‡n trong ná»™i dung vÃ  Ã½ nghÄ©a cá»§a chÃºng Ä‘á»‘i vá»›i cÃ¢u há»i.\n"
        f"3. Suy luáº­n logic: Dá»±a trÃªn thÃ´ng tin Ä‘Ã£ phÃ¢n tÃ­ch, Ä‘Æ°a ra láº­p luáº­n há»£p lÃ½ Ä‘á»ƒ giáº£i quyáº¿t cÃ¢u há»i. Náº¿u cáº§n, hÃ£y so sÃ¡nh, Ä‘á»‘i chiáº¿u hoáº·c suy ra tá»« cÃ¡c dá»¯ kiá»‡n.\n"
    )    
    
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 4000,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.5,  # Giáº£m temperature Ä‘á»ƒ tÄƒng tÃ­nh logic
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def generate_keywords(query, context="", history_keywords=None):
    if history_keywords is None:
        history_keywords = set()
    prompt = (
        f"CÃ¢u há»i: {query}\nThÃ´ng tin hiá»‡n cÃ³: {context[:2000]}\n"
        f"Lá»‹ch sá»­ tá»« khÃ³a Ä‘Ã£ dÃ¹ng: {', '.join(history_keywords)}\n"
        f"HÃ£y suy luáº­n vÃ  táº¡o 2-3 tá»« khÃ³a má»›i, khÃ´ng trÃ¹ng vá»›i lá»‹ch sá»­, liÃªn quan Ä‘áº¿n cÃ¢u há»i. "
        f"Tráº£ vá» dÆ°á»›i dáº¡ng danh sÃ¡ch: * \"tá»« khÃ³a 1\" * \"tá»« khÃ³a 2\" * \"tá»« khÃ³a 3\"."
    )
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 500,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None
    