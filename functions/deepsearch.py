
from duckduckgo_search import DDGS
from bs4 import BeautifulSoup

import requests
from rich.markdown import Markdown



# import scripts
from commands import *
from file import *
from image import *
from generate import *

# Kh·ªüi t·∫°o console t·ª´ Rich
console = Console()
history_analys = []


import random                                                                  
                                                                            
def random_number(min, max):                                                           
    """                                                                          
    H√†m n√†y t·∫°o ra m·ªôt s·ªë ng·∫´u nhi√™n t·ª´ 10 ƒë·∫øn 25.                               
    """                                                                          
    return random.randint(min, max)     

max_results = random_number(10, 25) 

def search_web(query, max_results=max_results):
    results = []
    with DDGS() as ddgs:
        for r in ddgs.text(query, max_results=max_results):
            results.append({
                'title': r['title'],
                'url': r['href'],
                'snippet': r['body']
            })
    return results

def extract_content(url, snippet=""):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        tags_to_extract = ['p', 'h1', 'h2', 'h3', 'li']
        content_parts = []
        for tag in soup.find_all(tags_to_extract):
            text = tag.get_text(strip=True)
            if text:
                content_parts.append(text)
        content = f"Snippet: {snippet}\n" + " ".join(content_parts)
        return content
    except requests.RequestException as e:
        return f"Error fetching {url}: {str(e)}"

def extract_hrefs(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        href_list = [tag['href'] for tag in soup.find_all('a', href=True)]
        return href_list
    except requests.RequestException as e:
        return f"Error fetching {url}: {str(e)}"


def extract_queries(text, history_queries=None):
    if history_queries is None:
        history_queries = set()
    try:
        lines = text.split('\n')
    except AttributeError:
        text = ''.join(text)
        lines = text.split('\n')

    queries = set()
    in_query_section = False

    for i, line in enumerate(lines):
        line_clean = line.strip().lower()
        if line_clean.startswith('ƒë·ªÅ xu·∫•t truy v·∫•n:') or line_clean.startswith('**ƒë·ªÅ xu·∫•t truy v·∫•n:**'):
            in_query_section = True
        elif in_query_section and (not line.strip() or not line.strip().startswith('*')):
            in_query_section = False

        if in_query_section:
            if i + 1 < len(lines) and not lines[i].strip().startswith('*') and lines[i].strip().startswith('ƒê·ªÅ xu·∫•t truy v·∫•n:'):
                next_line = lines[i + 1].strip()
                if next_line and not next_line.startswith('Truy v·∫•n t·ª´') and not next_line.startswith('ƒê√°nh gi√°:'):
                    clean_query = next_line.strip('"').strip('*').strip()
                    if clean_query and clean_query not in history_queries:
                        queries.add(clean_query)
            elif line.strip().startswith('*'):
                clean_query = line.strip()[1:].strip().strip('"').strip()
                if clean_query and clean_query not in history_queries:
                    queries.add(clean_query)

    return list(queries)[:1]


def deepsearch(initial_query, max_iterations=random_number(2, 5)):
    current_queries = []
    accumulated_context = ""
    all_answers = {}
    all_data = ""
    history_queries = set([initial_query])
    history_keywords = set()
    keywords = generate_keywords(initial_query, history_keywords=history_keywords)
    history_keywords.update(keywords)

    ###

    iteration = 0
    processed_urls = set()  # Danh s√°ch ƒë·ªÉ l∆∞u c√°c URL ƒë√£ ph√¢n t√≠ch

    ### ph√¢n t√≠ch c√¢u h·ªèi
    analys_question_stream = analys_question(initial_query)
    full_analys_question=""
    with console.status("[bold green]Ph√¢n t√≠ch v·∫•n ƒë·ªÅ... [/bold green]", spinner="dots"):
        for part in analys_question_stream:
            if part is not None:
                full_analys_question += part
    console.print(Markdown(full_analys_question), soft_wrap=True, end="")

    if "Kh√≥ nha bro" in full_analys_question:
        all_answers.clear()
        history_analys.clear()
        console.print(f"\n[red]Kh√¥ng t√¨m th·∫•y th√¥ng tin, ƒë·ªÉ t√¥i ph√¢n t√≠ch l·∫°i c√¢u h·ªèi...[/red]\n")
        better_question_prompt = better_question(initial_query)
        
        new_question = ""
        with console.status("[bold green]Ph√¢n t√≠ch c√¢u h·ªèi... [/bold green]", spinner="dots"):
            for part in better_question_prompt:
                if part is not None:
                    new_question += part

        console.print(Markdown(new_question), soft_wrap=True, end="")
        full_analys_question = new_question
    
    history_analys.append(full_analys_question)
    ###

    ### ph√¢n t√≠ch t√¨m ki·∫øm t·∫°o c√¢u truy v·∫•n
    analys_prompt_stream = analys_prompt(history_analys)
    full_analys_prompt=""
    with console.status("[bold green]T√¨m ki·∫øm... [/bold green]", spinner="dots"):
        for part in analys_prompt_stream:
            if part is not None:
                full_analys_prompt += part
    final_analys_prompt = full_analys_prompt.strip('"')  # Lo·∫°i b·ªè d·∫•u ngo·∫∑c k√©p n·∫øu c√≥
    current_queries.append(final_analys_prompt)
    ###

    while iteration < max_iterations and current_queries:
        current_query = current_queries.pop(0)
        
        console.print("\n")
        console.print(f"[cyan]T√¨m ki·∫øm: {current_query}[/cyan]")
        console.print("\n")

        search_results = search_web(current_query)
        console.print(f"[yellow]T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£.[/yellow]")

        if not search_results:
            all_answers.clear()
            console.print("\n")
            console.print(f"[red]Kh√≥ nha bro! Kh√¥ng t√¨m th·∫•y th√¥ng tin, ƒë·ªÉ t√¥i ph√¢n t√≠ch l·∫°i c√¢u h·ªèi...[/red]")
            console.print("\n")
            return deepsearch(initial_query)
        
        if any(result.get('title', '').startswith('EOF') for result in search_results):
            all_answers.clear()
            console.print("\n")
            console.print(f"[red]Kh√≥ nha bro! Kh√¥ng t√¨m th·∫•y th√¥ng tin, ƒë·ªÉ t√¥i ph√¢n t√≠ch l·∫°i c√¢u h·ªèi...[/red]")
            console.print("\n")
            return deepsearch(initial_query)


        new_query_found = False

        result_processed = False
        new_query_found = False
    
        for i, result in enumerate(search_results):
            url = result['url']
            
            if url in processed_urls:
                continue
            ### content trong url
            content = extract_content(url)
            ### 

            if "Error" in content:
                continue
            
            analysis = process_link(initial_query, url, content, keywords)
            console.print("\n")
            final_analysis = ""
            with console.status(Markdown(f"T√¨m ki·∫øm trong [{result['title']}]({url})"), spinner="dots"):
                for part in analysis:
                    if part is not None:
                        final_analysis += part
            
            console.print(Markdown(f"T√¨m ki·∫øm trong [{result['title']}]({url})\n"), soft_wrap=True, end="")
            processed_urls.add(url)
            console.print(Markdown(final_analysis), soft_wrap=True, end="")
            
            # ƒê√°nh gi√° th√¥ng tin b·∫±ng process_link
            sufficiency_prompt = (
                f"Url: {url}\n"
                f"N·ªôi dung ph√¢n t√≠ch: {final_analysis}\n"
                f"C√¢u h·ªèi ban ƒë·∫ßu: {initial_query}\n"
                f"N·∫øu '{url}' n√†y tr√πng v·ªõi b·∫•t k·ª≥ URL n√†o trong danh s√°ch '{', '.join(processed_urls)}', tr·∫£ l·ªùi 'NOT YET' v√† kh√¥ng ƒë√°nh gi√° th√™m.\n"
                f"N·∫øu kh√¥ng tr√πng, h√£y ƒë√°nh gi√° xem th√¥ng tin n√†y ƒë√£ ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi ch∆∞a.\n"
                f"Tr·∫£ l·ªùi 'OK' n·∫øu ƒë·ªß, 'NOT YET' n·∫øu ch∆∞a ƒë·ªß."
            )
            
            sufficiency_analysis = process_link(initial_query, url, sufficiency_prompt, keywords)
            console.print("\n")
            sufficiency_result = ""
            for part in sufficiency_analysis:
                if part is not None:
                    sufficiency_result += part
            
            # Ki·ªÉm tra k·∫øt qu·∫£ ƒë√°nh gi√°
            if "OK" in sufficiency_result.upper():
                result_processed = True
                all_answers[initial_query] = final_analysis
                history_analys.append(final_analysis)
                all_data += f"{result['url']}: {final_analysis}\n"           
            else:
                result_processed = False


            # Tr√≠ch xu·∫•t new_query
            new_queries = extract_queries(final_analysis, history_queries)

            if new_queries:
                for query in new_queries:
                    if query not in history_queries:
                        current_queries.append(query)
                        history_queries.add(query)
                        console.print(f"Th√™m truy v·∫•n m·ªõi: {query}")
                        new_query_found = True
            
            accumulated_context += f"\nNgu·ªìn: {url}\n{content}\n"
            if result_processed or new_query_found:
                break
        

        # Thu th·∫≠p to√†n b·ªô ph·∫£n h·ªìi t·ª´ reason_with_ollama
        answer_stream = reason_with_ollama(initial_query, accumulated_context)
        full_answer = ""
        with console.status("[bold green]Suy lu·∫≠n... [/bold green]", spinner="dots"):
            for part in answer_stream:
                if part is not None:
                    full_answer += part
        all_answers[current_query] = full_answer
        history_analys.append(full_answer)
        console.print(Markdown(full_answer), soft_wrap=True, end="")

        new_queries_from_reasoning = extract_queries(full_answer)

        # Thu th·∫≠p to√†n b·ªô ph·∫£n h·ªìi t·ª´ evaluate_answer
        evaluation_stream = evaluate_answer(initial_query, full_answer, processed_urls)
        full_evaluation = ""
        with console.status("[bold green]ƒê√°nh gi√° n·ªôi dung... [/bold green]", spinner="dots"):
            for part in evaluation_stream:
                if part is not None:
                    full_evaluation += part
        
        if "ƒë√£ ƒë·ªß" in full_evaluation.lower():
            break
        elif "ch∆∞a ƒë·ªß" in full_evaluation.lower():
            new_queries_from_evaluation = extract_queries(full_evaluation)
            relevant_query = new_queries_from_evaluation[0] if new_queries_from_evaluation else (new_queries_from_reasoning[0] if new_queries_from_reasoning else None)
            if relevant_query and relevant_query not in current_queries and relevant_query not in all_answers:
                current_queries.append(relevant_query)
            iteration += 1
        else:
            new_queries_from_evaluation = extract_queries(full_evaluation)
            relevant_query = new_queries_from_evaluation[0] if new_queries_from_evaluation else (new_queries_from_reasoning[0] if new_queries_from_reasoning else None)
            if relevant_query and relevant_query not in current_queries and relevant_query not in all_answers:
                current_queries.append(relevant_query)
            else:
                current_queries.append(current_query)
            iteration += 1
 
    if iteration >= max_iterations:
        console.print("\n[bold green]K·∫øt th√∫c DeepSearch! üåü\n[/bold green]")
    else:
        console.print("\n[bold green]K·∫øt th√∫c DeepSearch! üåü\n[/bold green]")


    summary_stream = summarize_answers(initial_query, history_analys)
    final_answer = ""
    with console.status("[bold green]T·ªïng h·ª£p... [/bold green]", spinner="dots"):
        for part in summary_stream:
            if part is not None:
                final_answer += part
    history_analys.clear()
    history_queries.clear()
    history_keywords.clear()
    all_answers.clear()
    console.clear()
    return f"\n{final_answer}"

                                                           
                                                                                                  
                                                                                                                       
                                                            
                     

# #H√†m test
# if __name__ == "__main__":
#     query = "C·∫≠p nh·∫≠t gi√∫p t√¥i nh·ªØng t·ª´ ng·ªØ hot trend c·ªßa gi·ªõi tr·∫ª genz Vi·ªát Nam ƒë·∫ßu nƒÉm 2025"
#     console.print(deepsearch(query))