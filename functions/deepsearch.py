from duckduckgo_search import DDGS
from bs4 import BeautifulSoup
import requests
from rich.console import Console
from rich.markdown import Markdown
from rich.live import Live
import random
from typing import List, Set, Dict

# Gi·∫£ ƒë·ªãnh c√°c module n√†y ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a
from functions.subfuncs.commands import *
from functions.subfuncs.file import *
from functions.subfuncs.image import *
from functions.subfuncs.generate import *

console = Console()


class DeepSearch:
    #random number

    def random_number(self, min_val: int, max_val: int) -> int:
        """T·∫°o s·ªë ng·∫´u nhi√™n trong kho·∫£ng min_val ƒë·∫øn max_val."""
        return random.randint(min_val, max_val)

    def __init__(self, initial_query: str, max_iterations: int = 5, max_results: int = random_number(5, 10, 25)):
        """
        Kh·ªüi t·∫°o ƒë·ªëi t∆∞·ª£ng DeepSearch v·ªõi c√¢u h·ªèi ban ƒë·∫ßu v√† c√°c tham s·ªë c·∫•u h√¨nh.
        
        Args:
            initial_query (str): C√¢u h·ªèi/truy v·∫•n ban ƒë·∫ßu.
            max_iterations (int): S·ªë l·∫ßn l·∫∑p t·ªëi ƒëa (m·∫∑c ƒë·ªãnh l√† 5).
            max_results (int): S·ªë k·∫øt qu·∫£ t√¨m ki·∫øm t·ªëi ƒëa m·ªói truy v·∫•n (m·∫∑c ƒë·ªãnh l√† 10).
        """
        self.initial_query = initial_query
        self.max_iterations = max_iterations
        self.max_results = max_results
        self.current_queries: List[str] = []
        self.accumulated_context: str = ""
        self.all_answers: Dict[str, str] = {}
        self.all_data: str = ""
        self.history_queries: Set[str] = {initial_query}
        self.history_keywords: Set[str] = set()
        self.processed_urls: Set[str] = set()
        self.history_analys: List[str] = []

        ### config live
        self.refresh_second = 10 
        self.vertical_overflow = "ellipsis" #"visible"


    def search_web(self, query: str) -> List[Dict[str, str]]:
        """T√¨m ki·∫øm tr√™n web b·∫±ng DuckDuckGo v√† tr·∫£ v·ªÅ danh s√°ch k·∫øt qu·∫£."""
        results = []
        with DDGS() as ddgs:
            for r in ddgs.text(query, max_results=self.max_results):
                results.append({
                    'title': r['title'],
                    'url': r['href'],
                    'snippet': r['body']
                })
        return results

    def extract_content(self, url: str, snippet: str = "") -> str:
        """Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL, bao g·ªìm ƒëo·∫°n tr√≠ch v√† vƒÉn b·∫£n t·ª´ c√°c th·∫ª HTML."""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            tags_to_extract = ['p', 'h1', 'h2', 'h3', 'a']
            content_parts = [tag.get_text(strip=True) for tag in soup.find_all(tags_to_extract) if tag.get_text(strip=True)]
            content = f"Snippet: {snippet}\n" + "\n".join(content_parts)
            return content
        except requests.RequestException as e:
            return f"Error fetching {url}: {str(e)}"

    def extract_hrefs(self, url: str) -> List[str]:
        """Tr√≠ch xu·∫•t t·∫•t c·∫£ li√™n k·∫øt (href) t·ª´ m·ªôt URL."""
        try:
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            href_list = [tag['href'] for tag in soup.find_all('a', href=True)]
            return href_list
        except requests.RequestException as e:
            return [f"Error fetching {url}: {str(e)}"]

    def extract_queries(self, text: str) -> List[str]:
        """Tr√≠ch xu·∫•t c√°c truy v·∫•n ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t t·ª´ vƒÉn b·∫£n ph√¢n t√≠ch."""
        queries = set()
        lines = text.split('\n')
        in_query_section = False
        for i, line in enumerate(lines):
            line_clean = line.strip().lower()
            if line_clean.startswith('ƒë·ªÅ xu·∫•t truy v·∫•n:') or line_clean.startswith('**ƒë·ªÅ xu·∫•t truy v·∫•n:**'):
                in_query_section = True
            elif in_query_section and (not line.strip() or not line.strip().startswith('*')):
                in_query_section = False
            if in_query_section:
                if i + 1 < len(lines) and not lines[i].strip().startswith('*') and lines[i].strip().startswith('ƒê·ªÅ xu·∫•t truy v·∫•n:'):
                    next_line = lines[i + 1].strip()
                    if next_line and not next_line.startswith('Truy v·∫•n t·ª´') and not next_line.startswith('ƒê√°nh gi√°:'):
                        clean_query = next_line.strip('"').strip('*').strip()
                        if clean_query and clean_query not in self.history_queries:
                            queries.add(clean_query)
                elif line.strip().startswith('*'):
                    clean_query = line.strip()[1:].strip().strip('"').strip()
                    if clean_query and clean_query not in self.history_queries:
                        queries.add(clean_query)
        return list(queries)[:1]

    def generate_keywords_and_analyze_question(self):
        """T·∫°o t·ª´ kh√≥a v√† ph√¢n t√≠ch c√¢u h·ªèi ban ƒë·∫ßu."""
        keywords_stream = generate_keywords(self.initial_query)
        full_keywords = ""
        for part in keywords_stream:
            if part is not None:
                full_keywords += part
        
        # L√†m s·∫°ch v√† tr√≠ch xu·∫•t t·ª´ kh√≥a t·ª´ ƒë·ªãnh d·∫°ng danh s√°ch
        keywords = []
        for line in full_keywords.splitlines():
            line = line.strip()
            if line.startswith('*'):  # Ch·ªâ l·∫•y d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng *
                keyword = line.strip('*').strip().strip('"').strip()  # Lo·∫°i b·ªè *, ", v√† kho·∫£ng tr·∫Øng th·ª´a
                if keyword:  # Ch·ªâ th√™m n·∫øu t·ª´ kh√≥a kh√¥ng r·ªóng
                    keywords.append(keyword)
        
        self.history_keywords.update(keywords)  # C·∫≠p nh·∫≠t set v·ªõi t·ª´ kh√≥a ƒë√£ l√†m s·∫°ch
        # console.print(f"[red][DEBUG]{self.history_keywords}[/red]")

        with Live(Markdown("Ch·ªù x√≠u...üñêÔ∏è"), refresh_per_second=self.refresh_second, console=console, vertical_overflow=self.vertical_overflow) as live:
            analysis_stream = analys_question(self.initial_query, self.history_keywords)
            full_analysis = ""
            
            for part in analysis_stream:
                if part is not None:
                    full_analysis += part
                    live.update(Markdown(f"\n{full_analysis}"))

        # console.print(Markdown(full_analysis), soft_wrap=True, end="")

        if "Kh√≥ nha bro" in full_analysis:
            self.all_answers.clear()
            better_question_stream = better_question(self.initial_query)
            new_question = ""
            with Live(Markdown("Ch·ªù x√≠u...üñêÔ∏è"), refresh_per_second=self.refresh_second, console=console, vertical_overflow=self.vertical_overflow) as live:
                for part in better_question_stream:
                    if part is not None:
                        new_question += part
                        live.update(Markdown(f"\n{new_question}"))
            full_analysis = new_question

        self.history_analys.append(full_analysis)

    def analyze_prompt(self):
        """Ph√¢n t√≠ch g·ª£i √Ω ƒë·ªÉ t·∫°o truy v·∫•n ƒë·∫ßu ti√™n."""
        analysis_stream = analys_prompt(self.history_analys)
        full_analysis = ""
        for part in analysis_stream:
            if part is not None:
                full_analysis += part
        final_query = full_analysis.strip('"')
        self.current_queries.append(final_query)


    def process_single_result(self, result: Dict[str, str]) -> bool:
        """X·ª≠ l√Ω m·ªôt k·∫øt qu·∫£ t√¨m ki·∫øm v√† tr·∫£ v·ªÅ li·ªáu n√≥ c√≥ ƒë·ªß th√¥ng tin kh√¥ng."""
        url = result['url']
        if url in self.processed_urls:
            return False

        content = self.extract_content(url, result['snippet'])

        if "Error" in content:
            return False

        # S·ª≠ d·ª•ng Live ƒë·ªÉ hi·ªÉn th·ªã c·∫£ tr·∫°ng th√°i v√† n·ªôi dung
        final_analysis = ""
        console.print("\n")
        status_text = f"T√¨m ki·∫øm trong [{result['title']}]({url}): "
        with Live(Markdown(status_text), refresh_per_second=self.refresh_second, console=console, vertical_overflow=self.vertical_overflow) as live:
            analysis_stream = process_link(self.initial_query, url, content, list(self.history_keywords))
            for part in analysis_stream:
                if part is not None:
                    final_analysis += part
                    live.update(Markdown(f"{status_text}\n\n{final_analysis}"))

        # Ph·∫ßn c√≤n l·∫°i gi·ªØ nguy√™n
        self.processed_urls.add(url)
        sufficiency_stream = sufficiency_prompt(query=self.initial_query, url=url, processed_urls=self.processed_urls, final_analysis=final_analysis)
        sufficiency_result = ""
        for part in sufficiency_stream:
            if part is not None:
                sufficiency_result += part

        if "OK" in sufficiency_result.upper():
            self.all_answers[self.initial_query] = final_analysis
            self.history_analys.append(final_analysis)
            self.all_data += f"{url}: {final_analysis}\n"
            return True

        new_queries = self.extract_queries(final_analysis)
        for query in new_queries:
            if query not in self.history_queries:
                self.current_queries.append(query)
                self.history_queries.add(query)

        self.accumulated_context += f"\nNgu·ªìn: {url}\n{content}\n"
        return False

    def search_and_process(self):
        """Th·ª±c hi·ªán t√¨m ki·∫øm v√† x·ª≠ l√Ω k·∫øt qu·∫£ trong t·ªëi ƒëa max_iterations l·∫ßn."""
        iteration = 0
        while iteration < self.max_iterations and self.current_queries:
            current_query = self.current_queries.pop(0)
            current_query_cleaned = re.sub(r'[\'"]', '', current_query)  # Lo·∫°i b·ªè d·∫•u nh√°y
            current_query_cleaned = re.sub(r'[^\w\s+-=/*]', '', current_query_cleaned, flags=re.UNICODE)  # Gi·ªØ ch·ªØ, s·ªë, kho·∫£ng tr·∫Øng v√† d·∫•u g·∫°ch ngang, h·ªó tr·ª£ Unicode
            current_query_cleaned = current_query_cleaned.strip()
            console.print(f"[cyan]\nƒêang t√¨m ki·∫øm: {current_query_cleaned}\n[/cyan]")

            search_results = self.search_web(current_query_cleaned)
            console.print(f"[yellow]T√¨m th·∫•y {len(search_results)} k·∫øt qu·∫£.[/yellow]")
            console.print("\n") 
            if not search_results or any(result.get('title', '').startswith('EOF') for result in search_results):
                self.all_answers.clear()
                console.print("[red]Kh√¥ng t√¨m th·∫•y th√¥ng tin h·ªØu √≠ch. ƒêang kh·ªüi ƒë·ªông l·∫°i v·ªõi truy v·∫•n m·ªõi...[/red]")
                self.generate_keywords_and_analyze_question()
                self.analyze_prompt()
                continue

            for result in search_results:
                if self.process_single_result(result):
                    break
            console.print("\n") 

            
            evaluation_stream = evaluate_answer(self.initial_query, self.all_data , self.processed_urls)
            full_evaluation = ""
            for part in evaluation_stream:
                if part is not None:
                    full_evaluation += part

            if "ƒë√£ ƒë·ªß" in full_evaluation.lower():
                full_answer = ""
                with Live(Markdown("\nƒêang suy lu·∫≠n..\n"), refresh_per_second=self.refresh_second, console=console, vertical_overflow=self.vertical_overflow) as live:
                    answer_stream = reason_with_ollama(self.initial_query, self.history_analys)
                    for part in answer_stream:
                        if part is not None:
                            full_answer += part
                            full_answers=full_answer.replace("<|begin_of_thought|>", "").replace("<|end_of_thought|>", "").replace("<|begin_of_solution|>", "").replace("<|end_of_solution|>", "")
                            live.update(Markdown(f"\n{full_answers}"))

                self.all_answers[current_query_cleaned] = full_answers
                self.history_analys.append(full_answers)
                break
            else:
                new_queries = self.extract_queries(full_evaluation) or self.extract_queries(full_answers)
                if new_queries:
                    relevant_query = new_queries[0]
                    if relevant_query not in self.current_queries and relevant_query not in self.all_answers:
                        self.current_queries.append(relevant_query)
            iteration += 1

    def summarize(self) -> str:
        """T·ªïng h·ª£p c√°c c√¢u tr·∫£ l·ªùi ƒë√£ thu th·∫≠p."""
        with Live(Markdown("Ch·ªù x√≠u...üñêÔ∏è"), refresh_per_second=self.refresh_second, console=console, vertical_overflow=self.vertical_overflow) as live:
            summary_stream = summarize_answers(self.initial_query, self.history_analys)
            final_answer = ""

            for part in summary_stream:
                if part is not None:
                    final_answer += part
                    live.update(Markdown(f"\n{final_answer}"))

        return final_answer

    def run(self) -> str:
        """Ch·∫°y to√†n b·ªô qu√° tr√¨nh t√¨m ki·∫øm s√¢u v√† tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi cu·ªëi c√πng."""
        self.generate_keywords_and_analyze_question()
        self.analyze_prompt()
        self.search_and_process()
        console.clear()
        final_answer = self.summarize()
        # X√≥a l·ªãch s·ª≠ ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ
        self.history_analys.clear()
        self.history_queries.clear()
        self.history_keywords.clear()
        self.all_answers.clear()
        return f"\n{final_answer}"

# # V√≠ d·ª• s·ª≠ d·ª•ng
# if __name__ == "__main__":
#     query = "C·∫≠p nh·∫≠t t·ª´ v·ª±ng hot trend m·ªõi c·ªßa genz Vi·ªát Nam ƒë·∫ßu nƒÉm 2025"
#     deep_search = DeepSearch(query)
#     console.print(deep_search.run())