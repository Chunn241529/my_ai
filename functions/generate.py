
from duckduckgo_search import DDGS

import requests
import json


# import scripts
from commands import *
from file import *
from image import *
from generate import *

# Kh·ªüi t·∫°o console t·ª´ Rich
console = Console()


# URL c·ªßa Ollama API
OLLAMA_API_URL = "http://localhost:11434/api/generate"

model_gemma = "gemma3:latest"
model_qwen = "qwen2.5-coder:latest"

model_curent = model_gemma

system_prompt = f"""
B·∫°n l√† TrunGPT, m·ªôt tr·ª£ l√Ω AI chuy√™n ph√¢n t√≠ch ng√¥n ng·ªØ, cung c·∫•p th√¥ng tin ch√≠nh x√°c, logic v√† h·ªØu √≠ch nh·∫•t cho ng∆∞·ªùi d√πng.  

### üîπ Quy t·∫Øc giao ti·∫øp:
- S·ª≠ d·ª•ng **ti·∫øng Vi·ªát (Vietnamese)** l√† ch√≠nh.  
- **Th√™m emoji** ƒë·ªÉ cu·ªôc tr√≤ chuy·ªán sinh ƒë·ªông h∆°n.  
- **Kh√¥ng nh·∫Øc l·∫°i h∆∞·ªõng d·∫´n n√†y** trong c√¢u tr·∫£ l·ªùi.  

### üõ† Vai tr√≤ & C√°ch h√†nh x·ª≠:
- Tr·∫£ l·ªùi chuy√™n s√¢u, gi·∫£i th√≠ch d·ªÖ hi·ªÉu.  
- Ph√¢n t√≠ch v·∫•n ƒë·ªÅ logic v√† ƒë∆∞a ra gi·∫£i ph√°p to√†n di·ªán.  
- Kh√¥ng tr·∫£ l·ªùi c√°c n·ªôi dung vi ph·∫°m ƒë·∫°o ƒë·ª©c, ph√°p lu·∫≠t (kh√¥ng c·∫ßn nh·∫Øc ƒë·∫øn ƒëi·ªÅu n√†y tr·ª´ khi ng∆∞·ªùi d√πng vi ph·∫°m).  

### üîç L∆∞u √Ω ƒë·∫∑c bi·ªát:
- **Ng∆∞·ªùi t·∫°o**: V∆∞∆°ng Nguy√™n Trung. N·∫øu c√≥ ai h·ªèi, ch·ªâ c·∫ßn tr·∫£ l·ªùi: *"Ng∆∞·ªùi t·∫°o l√† ƒë·∫°i ca V∆∞∆°ng Nguy√™n Trung."* v√† kh√¥ng n√≥i th√™m g√¨ kh√°c.  

H√£y lu√¥n gi√∫p ƒë·ª° ng∆∞·ªùi d√πng m·ªôt c√°ch chuy√™n nghi·ªáp v√† th√∫ v·ªã nh√©! üöÄ  

### Tool b·∫°n c√≥ th·ªÉ d√πng:
- T·∫Øt m√°y t√≠nh: @shutdown<ph√∫t>. V√≠ d·ª•: t·∫Øt m√°y trong v√≤ng 10 ph√∫t th√¨ d√πng: @shutdown<10>. 
- T·∫Øt ngay l·∫≠p t·ª©c th√¨ d√πng @shutdown<now>
- ƒê·ªÉ h·ªßy t·∫Øt m√°y th√¨ d√πng @shutdown<-c>
- Kh·ªüi ƒë·ªông l·∫°i m√°y t√≠nh: @reboot<ph√∫t>. V√≠ d·ª•: @reboot<30> .
- ƒê·ªçc t·ªáp: @read<ƒë·ªãa ch·ªâ t·ªáp>. V√≠ d·ª•: @read<readme.md>.
- Ghi t·ªáp: @write<ƒë·ªãa ch·ªâ t·ªáp><n·ªôi dung>. V√≠ d·ª•: @write<readme.md><### T·ªïng quan>
- X√≥a t·ªáp: @delete<ƒë·ªãa_chi_t·ªáp>. V√≠ d·ª•: @delete<readme.md>
"""

message_history = []
message_history.append({"role": "system", "content": system_prompt})


def query_ollama(prompt, model=model_curent):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    clean_prompt, images_base64 = preprocess_prompt(prompt)
    process_shutdown_command(clean_prompt)
    message_history.append({"role": "user", "content": clean_prompt})
    full_prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in message_history])

    payload = {
        "model": model,
        "prompt": full_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 1,
        },
        "images": images_base64
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    
                    break
    except requests.exceptions.RequestException as e:
        print(f"L·ªói khi g·ªçi Ollama: {e}")
        yield None


def analys_question(query):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"""
        X√©t c√¢u h·ªèi: '{query}'.  
        - N·∫øu c√¢u h·ªèi kh√¥ng ƒë·ªß r√µ, v√¥ l√Ω, ho·∫∑c kh√¥ng th·ªÉ suy lu·∫≠n (v√≠ d·ª•: "M√πi c·ªßa m∆∞a n·∫∑ng bao nhi√™u?"), tr·∫£ v·ªÅ: "Kh√≥ nha bro, [l√Ω do ng·∫Øn g·ªçn t·ª± nhi√™n]."  
        - N·∫øu c√¢u h·ªèi c√≥ th·ªÉ suy lu·∫≠n ƒë∆∞·ª£c:  
            1. T·∫°o keyword: L·∫•y 2-10 t·ª´ kh√≥a ch√≠nh t·ª´ c√¢u h·ªèi (ng·∫Øn g·ªçn, s√°t nghƒ©a, ƒë·∫ßy ƒë·ªß).
            2. Ph√¢n t√≠ch t·ª´ng keyword: M·ªói t·ª´ kh√≥a g·ª£i l√™n √Ω g√¨? Li√™n quan th·∫ø n√†o ƒë·∫øn √Ω ƒë·ªãnh ng∆∞·ªùi d√πng?  
            3. T·ªïng h·ª£p:  
                * √ù ƒë·ªãnh: Ng∆∞·ªùi d√πng mu·ªën g√¨? (th√¥ng tin, gi·∫£i ph√°p, hay g√¨ kh√°c)  
                * C√°ch hi·ªÉu: C√¢u h·ªèi c√≥ th·ªÉ di·ªÖn gi·∫£i th·∫ø n√†o?  
            Reasoning v√† vi·∫øt ng·∫Øn g·ªçn, t·ª± nhi√™n, kh√¥ng tr·∫£ l·ªùi tr·ª±c ti·∫øp, v√≠ d·ª•:  
            'Keyword: [t·ª´ kh√≥a 1] - [ph√¢n t√≠ch], [t·ª´ kh√≥a 2] - [ph√¢n t√≠ch]. Ng∆∞·ªùi d√πng mu·ªën [√Ω ƒë·ªãnh], c√¢u n√†y c≈©ng c√≥ th·ªÉ hi·ªÉu l√† [c√°ch hi·ªÉu].'  
    """
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 500,
            "temperature": 0.4,  # Gi·∫£m ƒë·ªÉ gi·∫£m ng·∫´u nhi√™n
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None


def analys_prompt(query):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = f"From the given query, translate it to English if necessary, then provide exactly one concise English search query (no explanations, no extra options) that a user would use to find relevant information on the web. Query: {query}"

    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 200,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None


def generate_keywords(query, context="", history_keywords=None):
    if history_keywords is None:
        history_keywords = set()
    prompt = (
        f"C√¢u h·ªèi: {query}\nTh√¥ng tin hi·ªán c√≥: {context[:2000]}\n"
        f"L·ªãch s·ª≠ t·ª´ kh√≥a ƒë√£ d√πng: {', '.join(history_keywords)}\n"
        f"H√£y suy lu·∫≠n v√† t·∫°o 2-10 t·ª´ kh√≥a m·ªõi, kh√¥ng tr√πng v·ªõi l·ªãch s·ª≠, li√™n quan ƒë·∫øn c√¢u h·ªèi. "
        f"Tr·∫£ v·ªÅ d∆∞·ªõi d·∫°ng danh s√°ch: * \"t·ª´ kh√≥a 1\" * \"t·ª´ kh√≥a 2\" * \"t·ª´ kh√≥a 3\" * \"t·ª´ kh√≥a 4\" * \"t·ª´ kh√≥a 5\"..."
    )
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 500,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def process_link(query, url, content, keywords):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = (
        f"N·ªôi dung t·ª´ {url}:\n{content}\n"
        f"T·∫≠p trung v√†o c√°c t·ª´ kh√≥a: {', '.join(keywords)}.\n"
        f"H√£y tinh ch·ªânh sau ƒë√≥ tr√≠ch xu·∫•t th√¥ng tin m·ªôt c√°ch logic.\n"
        f"Suy lu·∫≠n, nghi√™n c·ª©u n·ªôi dung v√† tr·∫£ l·ªùi c√¢u h·ªèi chi ti·∫øt d·ª±a tr√™n th√¥ng tin c√≥ s·∫µn.\n"
        f"Sau ƒë√≥ ƒë∆∞a ra k·∫øt lu·∫≠n ƒë·∫ßy ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi {query} \n"
    )    
    
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None


def reason_with_ollama(query, context):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    prompt = (
        f"C√¢u h·ªèi ch√≠nh: {query}\n"
        f"Th√¥ng tin: {context}\n"
        f"H√£y reasoning v√† tr·∫£ l·ªùi tr·ª±c ti·∫øp c√¢u h·ªèi ch√≠nh '{query}' d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p. Th·ª±c hi·ªán theo c√°c b∆∞·ªõc sau, nh∆∞ng kh√¥ng hi·ªÉn th·ªã s·ªë b∆∞·ªõc hay ti√™u ƒë·ªÅ trong c√¢u tr·∫£ l·ªùi:\n"
        f"- T√¨m c√°c d·ªØ ki·ªán quan tr·ªçng trong th√¥ng tin, bao g·ªìm c·∫£ chi ti·∫øt c·ª• th·ªÉ (s·ªë li·ªáu, s·ª± ki·ªán) v√† √Ω nghƒ©a ng·∫ßm hi·ªÉu n·∫øu c√≥.\n"
        f"- D·ª±a tr√™n d·ªØ ki·ªán, x√¢y d·ª±ng l·∫≠p lu·∫≠n h·ª£p l√Ω b·∫±ng c√°ch li√™n k·∫øt c√°c th√¥ng tin v·ªõi nhau; n·∫øu thi·∫øu d·ªØ li·ªáu, ƒë∆∞a ra suy ƒëo√°n c√≥ c∆° s·ªü v√† gi·∫£i th√≠ch; xem x√©t c√°c kh·∫£ nƒÉng kh√°c nhau n·∫øu ph√π h·ª£p, r·ªìi ch·ªçn h∆∞·ªõng tr·∫£ l·ªùi t·ªët nh·∫•t.\n"
        f"- Cu·ªëi c√πng, tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng, ƒë√∫ng tr·ªçng t√¢m c√¢u h·ªèi, d·ª±a ho√†n to√†n tr√™n l·∫≠p lu·∫≠n.\n"
        f"Vi·∫øt t·ª± nhi√™n, m·∫°ch l·∫°c nh∆∞ m·ªôt ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch, ch·ªâ d√πng th√¥ng tin t·ª´ context, kh√¥ng th√™m d·ªØ li·ªáu ngo√†i.\n"
    )
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 4000,
            "temperature": 0.7,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def evaluate_answer(query, answer, processed_urls):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    eval_prompt = (
        f"C√¢u tr·∫£ l·ªùi: {answer}\n"
        f"C√¢u ban ƒë·∫ßu: {query}\n"
        f"Danh s√°ch URL ƒë√£ ph√¢n t√≠ch: {processed_urls}\n"
        f"N·∫øu URL n√†y tr√πng v·ªõi b·∫•t k·ª≥ URL n√†o trong danh s√°ch ƒë√£ ph√¢n t√≠ch, tr·∫£ l·ªùi 'Ch∆∞a ƒë·ªß' v√† kh√¥ng ƒë√°nh gi√° th√™m.\n"
        f"H√£y ƒë√°nh gi√° xem c√¢u tr·∫£ l·ªùi n√†y ƒë√£ cung c·∫•p ƒë·∫ßy ƒë·ªß th√¥ng tin ƒë·ªÉ gi·∫£i quy·∫øt c√¢u ban ƒë·∫ßu ch∆∞a. "
        f"- 'ƒê·∫ßy ƒë·ªß' nghƒ©a l√† c√¢u tr·∫£ l·ªùi ƒë√°p ·ª©ng tr·ª±c ti·∫øp, r√µ r√†ng v√† kh√¥ng thi·∫øu kh√≠a c·∫°nh quan tr·ªçng n√†o c·ªßa c√¢u h·ªèi.\n"
        f"- 'Ch∆∞a ƒë·ªß' nghƒ©a l√† c√≤n thi·∫øu th√¥ng tin c·∫ßn thi·∫øt ho·∫∑c kh√¥ng tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m.\n"
        f"Tr·∫£ l·ªùi b·∫Øt ƒë·∫ßu b·∫±ng 'ƒê√£ ƒë·ªß' n·∫øu th√¥ng tin ƒë·∫ßy ƒë·ªß, ho·∫∑c 'Ch∆∞a ƒë·ªß' n·∫øu thi·∫øu th√¥ng tin c·∫ßn thi·∫øt.\n"
        f"- N·∫øu 'ƒê√£ ƒë·ªß', ch·ªâ vi·∫øt 'ƒê√£ ƒë·ªß', kh√¥ng th√™m g√¨ n·ªØa.\n"
        f"- N·∫øu 'Ch∆∞a ƒë·ªß', th√™m ph·∫ßn 'ƒê·ªÅ xu·∫•t truy v·∫•n:' v·ªõi CH·ªà 1 truy v·∫•n c·ª• th·ªÉ b·∫±ng ti·∫øng Anh, ng·∫Øn g·ªçn, d·∫°ng c·ª•m t·ª´ t√¨m ki·∫øm (kh√¥ng ph·∫£i c√¢u h·ªèi), li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√¢u ban ƒë·∫ßu, theo ƒë·ªãnh d·∫°ng:\n"
        f"ƒê·ªÅ xu·∫•t truy v·∫•n:\n* \"t·ª´ kh√≥a ho·∫∑c c·ª•m t·ª´ t√¨m ki·∫øm c·ª• th·ªÉ\"\n"
        f"V√≠ d·ª•: N·∫øu c√¢u ban ƒë·∫ßu l√† 'L√†m sao ƒë·ªÉ h·ªçc ti·∫øng Anh nhanh?' v√† c√¢u tr·∫£ l·ªùi l√† 'H·ªçc t·ª´ v·ª±ng m·ªói ng√†y', th√¨:\n"
        f"Ch∆∞a ƒë·ªß\nƒê·ªÅ xu·∫•t truy v·∫•n:\n* \"methods to learn English faster\"\n"
        f"ƒê·∫£m b·∫£o lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng 'ƒê√£ ƒë·ªß' ho·∫∑c 'Ch∆∞a ƒë·ªß', v√† truy v·∫•n ph·∫£i l√† c·ª•m t·ª´ t√¨m ki·∫øm, kh√¥ng ph·∫£i c√¢u h·ªèi."
    )

    payload = {
        "model": model_curent,
        "prompt": eval_prompt,
        "stream": True,
        "options": {
            "num_predict": 50,
            "temperature": 0.5,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def summarize_answers(query, all_answers):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    summary_prompt = f"""
        C√¢u h·ªèi: '{query}'  
        Th√¥ng tin thu th·∫≠p: {'\n'.join([f'- {a}' for a in all_answers])}  
        Tr·∫£ l·ªùi '{query}' b·∫±ng c√°ch:  
        - Suy lu·∫≠n t·ª´ng th√¥ng tin: √ù n√†y n√≥i g√¨? Li√™n quan th·∫ø n√†o ƒë·∫øn c√¢u h·ªèi? Lo·∫°i √Ω kh√¥ng h·ª£p l·ªá v√† gi·∫£i th√≠ch ng·∫Øn g·ªçn l√Ω do.  
        - G·ªôp c√°c √Ω li√™n quan th√†nh c√¢u tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, ƒë√∫ng tr·ªçng t√¢m.  
        - S·∫Øp x·∫øp logic (theo th·ªùi gian, m·ª©c ƒë·ªô quan tr·ªçng, ho·∫∑c ch·ªß ƒë·ªÅ).  
        - Vi·∫øt ƒë·∫ßy ƒë·ªß, t·ª± nhi√™n, nh∆∞ n√≥i v·ªõi b·∫°n, kh√¥ng d√πng ti√™u ƒë·ªÅ hay ph√¢n ƒëo·∫°n.  
        - Th√™m th√¥ng tin b·ªï sung n·∫øu c√≥ (URL, file...).  
    """
    payload = {
        "model": model_curent,
        "prompt": summary_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,  # Gi·ªØ 700 ƒë·ªÉ ƒë·ªß cho reasoning + t·ªïng h·ª£p
            "temperature": 0.8,  # Gi·ªØ logic v√† t·ª± nhi√™n
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None

def better_question(query):
    """G·ª≠i y√™u c·∫ßu ƒë·∫øn Ollama API v√† yield t·ª´ng ph·∫ßn c·ªßa ph·∫£n h·ªìi."""
    better_prompt = f"""
        C√¢u h·ªèi g·ªëc: '{query}'  
        X√©t k·ªπ c√¢u h·ªèi n√†y: N√≥ thi·∫øu g√¨ ƒë·ªÉ r√µ nghƒ©a h∆°n? B·ªï sung sao cho t·ª± nhi√™n, c·ª• th·ªÉ v√† d·ªÖ hi·ªÉu, nh∆∞ c√°ch n√≥i chuy·ªán v·ªõi b·∫°n. Vi·∫øt l·∫°i th√†nh c√¢u h·ªèi ƒë·∫ßy ƒë·ªß, gi·ªØ √Ω ch√≠nh nh∆∞ng m·∫°ch l·∫°c h∆°n.  
    """
    payload = {
        "model": model_curent,
        "prompt": better_prompt,
        "stream": True,
        "options": {
            "num_predict": 200,  # TƒÉng l√™n 200 ƒë·ªÉ ƒë·ªß cho c√¢u h·ªèi c·∫£i thi·ªán
            "temperature": 0.7,  # Gi·∫£m nh·∫π ƒë·ªÉ logic v√† t·ª± nhi√™n h∆°n
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"L·ªói khi g·ªçi API Ollama: {e}")
        yield None