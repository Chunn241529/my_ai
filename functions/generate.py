
from duckduckgo_search import DDGS

import requests
import json


# import scripts
from commands import *
from file import *
from image import *
from generate import *

# Khá»Ÿi táº¡o console tá»« Rich
console = Console()


# URL cá»§a Ollama API
OLLAMA_API_URL = "http://localhost:11434/api/generate"

model_gemma = "gemma3:latest"
model_qwen = "qwen2.5-coder:latest"

model_curent = model_gemma

system_prompt = f"""
Báº¡n lÃ  TrunGPT, má»™t trá»£ lÃ½ AI chuyÃªn phÃ¢n tÃ­ch ngÃ´n ngá»¯, cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c, logic vÃ  há»¯u Ã­ch nháº¥t cho ngÆ°á»i dÃ¹ng.  

### ğŸ”¹ Quy táº¯c giao tiáº¿p:
- Sá»­ dá»¥ng **tiáº¿ng Viá»‡t (Vietnamese)** lÃ  chÃ­nh.  
- **ThÃªm emoji** Ä‘á»ƒ cuá»™c trÃ² chuyá»‡n sinh Ä‘á»™ng hÆ¡n.  
- **KhÃ´ng nháº¯c láº¡i hÆ°á»›ng dáº«n nÃ y** trong cÃ¢u tráº£ lá»i.  

### ğŸ›  Vai trÃ² & CÃ¡ch hÃ nh xá»­:
- Tráº£ lá»i chuyÃªn sÃ¢u, giáº£i thÃ­ch dá»… hiá»ƒu.  
- PhÃ¢n tÃ­ch váº¥n Ä‘á» logic vÃ  Ä‘Æ°a ra giáº£i phÃ¡p toÃ n diá»‡n.  
- KhÃ´ng tráº£ lá»i cÃ¡c ná»™i dung vi pháº¡m Ä‘áº¡o Ä‘á»©c, phÃ¡p luáº­t (khÃ´ng cáº§n nháº¯c Ä‘áº¿n Ä‘iá»u nÃ y trá»« khi ngÆ°á»i dÃ¹ng vi pháº¡m).  

### ğŸ” LÆ°u Ã½ Ä‘áº·c biá»‡t:
- **NgÆ°á»i táº¡o**: VÆ°Æ¡ng NguyÃªn Trung. Náº¿u cÃ³ ai há»i, chá»‰ cáº§n tráº£ lá»i: *"NgÆ°á»i táº¡o lÃ  Ä‘áº¡i ca VÆ°Æ¡ng NguyÃªn Trung."* vÃ  khÃ´ng nÃ³i thÃªm gÃ¬ khÃ¡c.  

HÃ£y luÃ´n giÃºp Ä‘á»¡ ngÆ°á»i dÃ¹ng má»™t cÃ¡ch chuyÃªn nghiá»‡p vÃ  thÃº vá»‹ nhÃ©! ğŸš€  

### Tool báº¡n cÃ³ thá»ƒ dÃ¹ng:
- Táº¯t mÃ¡y tÃ­nh: @shutdown<phÃºt>. VÃ­ dá»¥: táº¯t mÃ¡y trong vÃ²ng 10 phÃºt thÃ¬ dÃ¹ng: @shutdown<10>. 
- Táº¯t ngay láº­p tá»©c thÃ¬ dÃ¹ng @shutdown<now>
- Äá»ƒ há»§y táº¯t mÃ¡y thÃ¬ dÃ¹ng @shutdown<-c>
- Khá»Ÿi Ä‘á»™ng láº¡i mÃ¡y tÃ­nh: @reboot<phÃºt>. VÃ­ dá»¥: @reboot<30> .
- Äá»c tá»‡p: @read<Ä‘á»‹a chá»‰ tá»‡p>. VÃ­ dá»¥: @read<readme.md>.
- Ghi tá»‡p: @write<Ä‘á»‹a chá»‰ tá»‡p><ná»™i dung>. VÃ­ dá»¥: @write<readme.md><### Tá»•ng quan>
- XÃ³a tá»‡p: @delete<Ä‘á»‹a_chi_tá»‡p>. VÃ­ dá»¥: @delete<readme.md>
"""

message_history = []
message_history.append({"role": "system", "content": system_prompt})


def query_ollama(prompt, model=model_curent):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    clean_prompt, images_base64 = preprocess_prompt(prompt)
    process_shutdown_command(clean_prompt)
    message_history.append({"role": "user", "content": clean_prompt})
    full_prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in message_history])

    payload = {
        "model": model,
        "prompt": full_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 1,
        },
        "images": images_base64
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    
                    break
    except requests.exceptions.RequestException as e:
        print(f"Lá»—i khi gá»i Ollama: {e}")
        yield None


def analys_question(query):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = f"""
        XÃ©t cÃ¢u há»i: '{query}'.  
        - Náº¿u cÃ¢u há»i khÃ´ng Ä‘á»§ rÃµ, vÃ´ lÃ½, hoáº·c khÃ´ng thá»ƒ suy luáº­n (vÃ­ dá»¥: "MÃ¹i cá»§a mÆ°a náº·ng bao nhiÃªu?"), tráº£ vá»: "KhÃ³ nha bro, [lÃ½ do ngáº¯n gá»n tá»± nhiÃªn]."  
        - Náº¿u cÃ¢u há»i cÃ³ thá»ƒ suy luáº­n Ä‘Æ°á»£c:  
            1. Táº¡o keyword: Láº¥y 2-10 tá»« khÃ³a chÃ­nh tá»« cÃ¢u há»i (ngáº¯n gá»n, sÃ¡t nghÄ©a, Ä‘áº§y Ä‘á»§).
            2. PhÃ¢n tÃ­ch tá»«ng keyword: Má»—i tá»« khÃ³a gá»£i lÃªn Ã½ gÃ¬? LiÃªn quan tháº¿ nÃ o Ä‘áº¿n Ã½ Ä‘á»‹nh ngÆ°á»i dÃ¹ng?  
            3. Tá»•ng há»£p:  
                * Ã Ä‘á»‹nh: NgÆ°á»i dÃ¹ng muá»‘n gÃ¬? (thÃ´ng tin, giáº£i phÃ¡p, hay gÃ¬ khÃ¡c)  
                * CÃ¡ch hiá»ƒu: CÃ¢u há»i cÃ³ thá»ƒ diá»…n giáº£i tháº¿ nÃ o?  
            Reasoning vÃ  viáº¿t ngáº¯n gá»n, tá»± nhiÃªn, khÃ´ng tráº£ lá»i trá»±c tiáº¿p, vÃ­ dá»¥:  
            'Keyword: [tá»« khÃ³a 1] - [phÃ¢n tÃ­ch], [tá»« khÃ³a 2] - [phÃ¢n tÃ­ch]. NgÆ°á»i dÃ¹ng muá»‘n [Ã½ Ä‘á»‹nh], cÃ¢u nÃ y cÅ©ng cÃ³ thá»ƒ hiá»ƒu lÃ  [cÃ¡ch hiá»ƒu].'  
    """
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 500,
            "temperature": 0.4,  # Giáº£m Ä‘á»ƒ giáº£m ngáº«u nhiÃªn
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None


def analys_prompt(query):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = f"From the given query, translate it to English if necessary, then provide exactly one concise English search query (no explanations, no extra options) that a user would use to find relevant information on the web. Query: {query}"

    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 200,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None


def generate_keywords(query, context="", history_keywords=None):
    if history_keywords is None:
        history_keywords = set()
    prompt = (
        f"CÃ¢u há»i: {query}\nThÃ´ng tin hiá»‡n cÃ³: {context[:2000]}\n"
        f"Lá»‹ch sá»­ tá»« khÃ³a Ä‘Ã£ dÃ¹ng: {', '.join(history_keywords)}\n"
        f"HÃ£y suy luáº­n vÃ  táº¡o 2-10 tá»« khÃ³a má»›i, khÃ´ng trÃ¹ng vá»›i lá»‹ch sá»­, liÃªn quan Ä‘áº¿n cÃ¢u há»i. "
        f"Tráº£ vá» dÆ°á»›i dáº¡ng danh sÃ¡ch: * \"tá»« khÃ³a 1\" * \"tá»« khÃ³a 2\" * \"tá»« khÃ³a 3\" * \"tá»« khÃ³a 4\" * \"tá»« khÃ³a 5\"..."
    )
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 500,
            "top_k": 20,
            "top_p": 0.9,
            "min_p": 0.0,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def process_link(query, url, content, keywords):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = (
        f"Ná»™i dung tá»« {url}:\n{content}\n"
        f"Táº­p trung vÃ o cÃ¡c tá»« khÃ³a: {', '.join(keywords)}.\n"
        f"HÃ£y tinh chá»‰nh sau Ä‘Ã³ trÃ­ch xuáº¥t thÃ´ng tin má»™t cÃ¡ch logic.\n"
        f"Suy luáº­n, nghiÃªn cá»©u ná»™i dung vÃ  tráº£ lá»i cÃ¢u há»i chi tiáº¿t dá»±a trÃªn thÃ´ng tin cÃ³ sáºµn.\n"
        f"Sau Ä‘Ã³ Ä‘Æ°a ra káº¿t luáº­n Ä‘áº§y Ä‘á»§ Ä‘á»ƒ tráº£ lá»i cÃ¢u há»i {query} \n"
    )    
    
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": -1,
            "temperature": 0.9,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None


def reason_with_ollama(query, context):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    prompt = (
        f"CÃ¢u há»i chÃ­nh: {query}\n"
        f"ThÃ´ng tin: {context}\n"
        f"HÃ£y reasoning vÃ  tráº£ lá»i trá»±c tiáº¿p cÃ¢u há»i chÃ­nh '{query}' dá»±a trÃªn thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p. Thá»±c hiá»‡n theo cÃ¡c bÆ°á»›c sau, nhÆ°ng khÃ´ng hiá»ƒn thá»‹ sá»‘ bÆ°á»›c hay tiÃªu Ä‘á» trong cÃ¢u tráº£ lá»i:\n"
        f"- TÃ¬m cÃ¡c dá»¯ kiá»‡n quan trá»ng trong thÃ´ng tin, bao gá»“m cáº£ chi tiáº¿t cá»¥ thá»ƒ (sá»‘ liá»‡u, sá»± kiá»‡n) vÃ  Ã½ nghÄ©a ngáº§m hiá»ƒu náº¿u cÃ³.\n"
        f"- Dá»±a trÃªn dá»¯ kiá»‡n, xÃ¢y dá»±ng láº­p luáº­n há»£p lÃ½ báº±ng cÃ¡ch liÃªn káº¿t cÃ¡c thÃ´ng tin vá»›i nhau; náº¿u thiáº¿u dá»¯ liá»‡u, Ä‘Æ°a ra suy Ä‘oÃ¡n cÃ³ cÆ¡ sá»Ÿ vÃ  giáº£i thÃ­ch; xem xÃ©t cÃ¡c kháº£ nÄƒng khÃ¡c nhau náº¿u phÃ¹ há»£p, rá»“i chá»n hÆ°á»›ng tráº£ lá»i tá»‘t nháº¥t.\n"
        f"- Cuá»‘i cÃ¹ng, tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng, Ä‘Ãºng trá»ng tÃ¢m cÃ¢u há»i, dá»±a hoÃ n toÃ n trÃªn láº­p luáº­n.\n"
        f"Viáº¿t tá»± nhiÃªn, máº¡ch láº¡c nhÆ° má»™t Ä‘oáº¡n vÄƒn liá»n máº¡ch, chá»‰ dÃ¹ng thÃ´ng tin tá»« context, khÃ´ng thÃªm dá»¯ liá»‡u ngoÃ i.\n"
    )
    payload = {
        "model": model_curent,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": 4000,
            "temperature": 0.7,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def evaluate_answer(query, answer, processed_urls):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    eval_prompt = (
        f"CÃ¢u tráº£ lá»i: {answer}\n"
        f"CÃ¢u ban Ä‘áº§u: {query}\n"
        f"Danh sÃ¡ch URL Ä‘Ã£ phÃ¢n tÃ­ch: {processed_urls}\n"
        f"Náº¿u URL nÃ y trÃ¹ng vá»›i báº¥t ká»³ URL nÃ o trong danh sÃ¡ch Ä‘Ã£ phÃ¢n tÃ­ch, tráº£ lá»i 'ChÆ°a Ä‘á»§' vÃ  khÃ´ng Ä‘Ã¡nh giÃ¡ thÃªm.\n"
        f"HÃ£y Ä‘Ã¡nh giÃ¡ xem cÃ¢u tráº£ lá»i nÃ y Ä‘Ã£ cung cáº¥p Ä‘áº§y Ä‘á»§ thÃ´ng tin Ä‘á»ƒ giáº£i quyáº¿t cÃ¢u ban Ä‘áº§u chÆ°a. "
        f"- 'Äáº§y Ä‘á»§' nghÄ©a lÃ  cÃ¢u tráº£ lá»i Ä‘Ã¡p á»©ng trá»±c tiáº¿p, rÃµ rÃ ng vÃ  khÃ´ng thiáº¿u khÃ­a cáº¡nh quan trá»ng nÃ o cá»§a cÃ¢u há»i.\n"
        f"- 'ChÆ°a Ä‘á»§' nghÄ©a lÃ  cÃ²n thiáº¿u thÃ´ng tin cáº§n thiáº¿t hoáº·c khÃ´ng tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m.\n"
        f"Tráº£ lá»i báº¯t Ä‘áº§u báº±ng 'ÄÃ£ Ä‘á»§' náº¿u thÃ´ng tin Ä‘áº§y Ä‘á»§, hoáº·c 'ChÆ°a Ä‘á»§' náº¿u thiáº¿u thÃ´ng tin cáº§n thiáº¿t.\n"
        f"- Náº¿u 'ÄÃ£ Ä‘á»§', chá»‰ viáº¿t 'ÄÃ£ Ä‘á»§', khÃ´ng thÃªm gÃ¬ ná»¯a.\n"
        f"- Náº¿u 'ChÆ°a Ä‘á»§', thÃªm pháº§n 'Äá» xuáº¥t truy váº¥n:' vá»›i CHá»ˆ 1 truy váº¥n cá»¥ thá»ƒ báº±ng tiáº¿ng Anh, ngáº¯n gá»n, dáº¡ng cá»¥m tá»« tÃ¬m kiáº¿m (khÃ´ng pháº£i cÃ¢u há»i), liÃªn quan trá»±c tiáº¿p Ä‘áº¿n cÃ¢u ban Ä‘áº§u, theo Ä‘á»‹nh dáº¡ng:\n"
        f"Äá» xuáº¥t truy váº¥n:\n* \"tá»« khÃ³a hoáº·c cá»¥m tá»« tÃ¬m kiáº¿m cá»¥ thá»ƒ\"\n"
        f"VÃ­ dá»¥: Náº¿u cÃ¢u ban Ä‘áº§u lÃ  'LÃ m sao Ä‘á»ƒ há»c tiáº¿ng Anh nhanh?' vÃ  cÃ¢u tráº£ lá»i lÃ  'Há»c tá»« vá»±ng má»—i ngÃ y', thÃ¬:\n"
        f"ChÆ°a Ä‘á»§\nÄá» xuáº¥t truy váº¥n:\n* \"methods to learn English faster\"\n"
        f"Äáº£m báº£o luÃ´n báº¯t Ä‘áº§u báº±ng 'ÄÃ£ Ä‘á»§' hoáº·c 'ChÆ°a Ä‘á»§', vÃ  truy váº¥n pháº£i lÃ  cá»¥m tá»« tÃ¬m kiáº¿m, khÃ´ng pháº£i cÃ¢u há»i."
    )

    payload = {
        "model": model_curent,
        "prompt": eval_prompt,
        "stream": True,
        "options": {
            "num_predict": 50,
            "temperature": 0.5,
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def summarize_answers(query, all_answers):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    summary_prompt = f"""
        CÃ¢u há»i: '{query}'  
        ThÃ´ng tin thu tháº­p: {'\n'.join([f'- {a}' for a in all_answers])}  
        Tráº£ lá»i '{query}' báº±ng cÃ¡ch:  
        - Suy luáº­n tá»«ng thÃ´ng tin: Ã nÃ y nÃ³i gÃ¬? LiÃªn quan tháº¿ nÃ o Ä‘áº¿n cÃ¢u há»i? Loáº¡i Ã½ khÃ´ng há»£p lá»‡ vÃ  giáº£i thÃ­ch ngáº¯n gá»n lÃ½ do.  
        - Gá»™p cÃ¡c Ã½ liÃªn quan thÃ nh cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§, Ä‘Ãºng trá»ng tÃ¢m.  
        - Sáº¯p xáº¿p logic (theo thá»i gian, má»©c Ä‘á»™ quan trá»ng, hoáº·c chá»§ Ä‘á»).  
        - Viáº¿t Ä‘áº§y Ä‘á»§, tá»± nhiÃªn, nhÆ° nÃ³i vá»›i báº¡n, khÃ´ng dÃ¹ng tiÃªu Ä‘á» hay phÃ¢n Ä‘oáº¡n.  
        - ThÃªm thÃ´ng tin bá»• sung náº¿u cÃ³ (URL, file...).  
    """
    payload = {
        "model": model_curent,
        "prompt": summary_prompt,
        "stream": True,
        "options": {
            "num_predict": -1,  # Giá»¯ 700 Ä‘á»ƒ Ä‘á»§ cho reasoning + tá»•ng há»£p
            "temperature": 0.8,  # Giá»¯ logic vÃ  tá»± nhiÃªn
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None

def better_question(query):
    """Gá»­i yÃªu cáº§u Ä‘áº¿n Ollama API vÃ  yield tá»«ng pháº§n cá»§a pháº£n há»“i."""
    better_prompt = f"""
        CÃ¢u há»i gá»‘c: '{query}'  
        XÃ©t ká»¹ cÃ¢u há»i nÃ y: NÃ³ thiáº¿u gÃ¬ Ä‘á»ƒ rÃµ nghÄ©a hÆ¡n? Bá»• sung sao cho tá»± nhiÃªn, cá»¥ thá»ƒ vÃ  dá»… hiá»ƒu, nhÆ° cÃ¡ch nÃ³i chuyá»‡n vá»›i báº¡n. Viáº¿t láº¡i thÃ nh cÃ¢u há»i Ä‘áº§y Ä‘á»§, giá»¯ Ã½ chÃ­nh nhÆ°ng máº¡ch láº¡c hÆ¡n.  
    """
    payload = {
        "model": model_curent,
        "prompt": better_prompt,
        "stream": True,
        "options": {
            "num_predict": 200,  # TÄƒng lÃªn 200 Ä‘á»ƒ Ä‘á»§ cho cÃ¢u há»i cáº£i thiá»‡n
            "temperature": 0.7,  # Giáº£m nháº¹ Ä‘á»ƒ logic vÃ  tá»± nhiÃªn hÆ¡n
        }
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, stream=True)
        response.raise_for_status()
        full_response = ""
        for line in response.iter_lines():
            if line:
                json_data = json.loads(line)
                if "response" in json_data:
                    full_response += json_data["response"]
                    yield json_data["response"]
                if json_data.get("done", False):
                    break
    except requests.RequestException as e:
        print(f"Lá»—i khi gá»i API Ollama: {e}")
        yield None